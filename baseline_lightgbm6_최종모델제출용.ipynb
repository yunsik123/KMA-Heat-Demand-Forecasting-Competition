{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import holidays\n",
    "from scipy.fftpack import fft#í‘¸ë¦¬ì— ë³€í™˜ì„ ìœ„í•œ ì½”ë“œì…ë‹ˆë‹¤.\n",
    "from scipy.stats import boxcox#ë°•ìŠ¤ì½•ìŠ¤ ë³€í™˜ì„ ìœ„í•œ ì½”ë“œì„\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# ===== LightGBM ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ =====\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.linear_model import Ridge\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "#ê¸°íƒ€\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ¢ ë¸Œëœì¹˜ A SVR ë³´ê°„ ì¤‘... âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ B SVR ë³´ê°„ ì¤‘... âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ C SVR ë³´ê°„ ì¤‘... âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ D SVR ë³´ê°„ ì¤‘... âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ E SVR ë³´ê°„ ì¤‘... âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ F SVR ë³´ê°„ ì¤‘... âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ G SVR ë³´ê°„ ì¤‘... âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ H SVR ë³´ê°„ ì¤‘... âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ I SVR ë³´ê°„ ì¤‘... âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ J SVR ë³´ê°„ ì¤‘... âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ K SVR ë³´ê°„ ì¤‘... âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ L SVR ë³´ê°„ ì¤‘... âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ M SVR ë³´ê°„ ì¤‘... âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ N SVR ë³´ê°„ ì¤‘... âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ O SVR ë³´ê°„ ì¤‘... âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ P SVR ë³´ê°„ ì¤‘... âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ Q SVR ë³´ê°„ ì¤‘... âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ R SVR ë³´ê°„ ì¤‘... âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ S SVR ë³´ê°„ ì¤‘... âœ…\n",
      "ğŸ‰ SVR ë³´ê°„ ì™„ë£Œ (ì¡°ê±´ ì—†ì´ ì „ë¶€ ì‹œë„)\n",
      "ğŸš€ STL ì‹œê³„ì—´ ë¶„í•´ ì‹œì‘...\n",
      "ğŸ”„ STL ì‹œê³„ì—´ ë¶„í•´ íŠ¹ì„± ìƒì„± ì¤‘... (ëŒ€ìƒ: ['ta', 'ws'])\n",
      "============================================================\n",
      "âš ï¸ heat_demandëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì—†ìœ¼ë¯€ë¡œ ì œì™¸ë©ë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ë¸Œëœì¹˜ë³„ STL ë¶„í•´: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [02:41<00:00,  8.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… STL ì‹œê³„ì—´ ë¶„í•´ ì™„ë£Œ!\n",
      "ğŸ“Š ìƒì„±ëœ ì‹œê³„ì—´ íŠ¹ì„±: 12ê°œ\n",
      "ğŸ“‹ íŠ¹ì„± ëª©ë¡: ['ta_stl_trend', 'ta_stl_seasonal', 'ta_stl_resid', 'ta_detrend', 'ta_seasonal_strength', 'ta_seasonal_volatility', 'ws_stl_trend', 'ws_stl_seasonal', 'ws_stl_resid', 'ws_detrend']...\n",
      "   ğŸ¢ ë¸Œëœì¹˜ A SVR ë³´ê°„ ì¤‘... \n",
      "   âš ï¸ heat_demand SVR ì‹¤íŒ¨ â†’ ì„ í˜• ë³´ê°„ ëŒ€ì²´\n",
      "âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ B SVR ë³´ê°„ ì¤‘... \n",
      "   âš ï¸ heat_demand SVR ì‹¤íŒ¨ â†’ ì„ í˜• ë³´ê°„ ëŒ€ì²´\n",
      "âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ C SVR ë³´ê°„ ì¤‘... \n",
      "   âš ï¸ heat_demand SVR ì‹¤íŒ¨ â†’ ì„ í˜• ë³´ê°„ ëŒ€ì²´\n",
      "âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ D SVR ë³´ê°„ ì¤‘... \n",
      "   âš ï¸ heat_demand SVR ì‹¤íŒ¨ â†’ ì„ í˜• ë³´ê°„ ëŒ€ì²´\n",
      "âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ E SVR ë³´ê°„ ì¤‘... \n",
      "   âš ï¸ heat_demand SVR ì‹¤íŒ¨ â†’ ì„ í˜• ë³´ê°„ ëŒ€ì²´\n",
      "âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ F SVR ë³´ê°„ ì¤‘... \n",
      "   âš ï¸ heat_demand SVR ì‹¤íŒ¨ â†’ ì„ í˜• ë³´ê°„ ëŒ€ì²´\n",
      "âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ G SVR ë³´ê°„ ì¤‘... \n",
      "   âš ï¸ heat_demand SVR ì‹¤íŒ¨ â†’ ì„ í˜• ë³´ê°„ ëŒ€ì²´\n",
      "âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ H SVR ë³´ê°„ ì¤‘... \n",
      "   âš ï¸ heat_demand SVR ì‹¤íŒ¨ â†’ ì„ í˜• ë³´ê°„ ëŒ€ì²´\n",
      "âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ I SVR ë³´ê°„ ì¤‘... \n",
      "   âš ï¸ heat_demand SVR ì‹¤íŒ¨ â†’ ì„ í˜• ë³´ê°„ ëŒ€ì²´\n",
      "âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ J SVR ë³´ê°„ ì¤‘... \n",
      "   âš ï¸ heat_demand SVR ì‹¤íŒ¨ â†’ ì„ í˜• ë³´ê°„ ëŒ€ì²´\n",
      "âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ K SVR ë³´ê°„ ì¤‘... \n",
      "   âš ï¸ heat_demand SVR ì‹¤íŒ¨ â†’ ì„ í˜• ë³´ê°„ ëŒ€ì²´\n",
      "âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ L SVR ë³´ê°„ ì¤‘... \n",
      "   âš ï¸ heat_demand SVR ì‹¤íŒ¨ â†’ ì„ í˜• ë³´ê°„ ëŒ€ì²´\n",
      "âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ M SVR ë³´ê°„ ì¤‘... \n",
      "   âš ï¸ heat_demand SVR ì‹¤íŒ¨ â†’ ì„ í˜• ë³´ê°„ ëŒ€ì²´\n",
      "âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ N SVR ë³´ê°„ ì¤‘... \n",
      "   âš ï¸ heat_demand SVR ì‹¤íŒ¨ â†’ ì„ í˜• ë³´ê°„ ëŒ€ì²´\n",
      "âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ O SVR ë³´ê°„ ì¤‘... \n",
      "   âš ï¸ heat_demand SVR ì‹¤íŒ¨ â†’ ì„ í˜• ë³´ê°„ ëŒ€ì²´\n",
      "âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ P SVR ë³´ê°„ ì¤‘... \n",
      "   âš ï¸ heat_demand SVR ì‹¤íŒ¨ â†’ ì„ í˜• ë³´ê°„ ëŒ€ì²´\n",
      "âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ Q SVR ë³´ê°„ ì¤‘... \n",
      "   âš ï¸ heat_demand SVR ì‹¤íŒ¨ â†’ ì„ í˜• ë³´ê°„ ëŒ€ì²´\n",
      "âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ R SVR ë³´ê°„ ì¤‘... \n",
      "   âš ï¸ heat_demand SVR ì‹¤íŒ¨ â†’ ì„ í˜• ë³´ê°„ ëŒ€ì²´\n",
      "âœ…\n",
      "   ğŸ¢ ë¸Œëœì¹˜ S SVR ë³´ê°„ ì¤‘... \n",
      "   âš ï¸ heat_demand SVR ì‹¤íŒ¨ â†’ ì„ í˜• ë³´ê°„ ëŒ€ì²´\n",
      "âœ…\n",
      "ğŸ‰ SVR ë³´ê°„ ì™„ë£Œ (ì¡°ê±´ ì—†ì´ ì „ë¶€ ì‹œë„)\n",
      "ğŸš€ STL ì‹œê³„ì—´ ë¶„í•´ ì‹œì‘...\n",
      "ğŸ”„ STL ì‹œê³„ì—´ ë¶„í•´ íŠ¹ì„± ìƒì„± ì¤‘... (ëŒ€ìƒ: ['ta', 'ws'])\n",
      "============================================================\n",
      "âš ï¸ heat_demandëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì—†ìœ¼ë¯€ë¡œ ì œì™¸ë©ë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ë¸Œëœì¹˜ë³„ STL ë¶„í•´: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:52<00:00,  2.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… STL ì‹œê³„ì—´ ë¶„í•´ ì™„ë£Œ!\n",
      "ğŸ“Š ìƒì„±ëœ ì‹œê³„ì—´ íŠ¹ì„±: 12ê°œ\n",
      "ğŸ“‹ íŠ¹ì„± ëª©ë¡: ['ta_stl_trend', 'ta_stl_seasonal', 'ta_stl_resid', 'ta_detrend', 'ta_seasonal_strength', 'ta_seasonal_volatility', 'ws_stl_trend', 'ws_stl_seasonal', 'ws_stl_resid', 'ws_detrend']...\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"train_heat.csv\")\n",
    "df_test = pd.read_csv(\"test_heat.csv\")\n",
    "#ì—´ì´ë¦„ë¹¼ê¸°\n",
    "df_train.columns = df_train.columns.str.replace('train_heat.', '', regex=False)\n",
    "#Unnamed:0ì œê±°\n",
    "df_train = df_train.drop(columns=[\"Unnamed: 0\"])\n",
    "#testë°ì´í„° ì—´ì´ë¦„ ë°”ê¾¸ê¸°\n",
    "df_test.columns = [\n",
    "    \"tm\", \"branch_id\", \"ta\", \"wd\", \"ws\",\n",
    "    \"rn_day\", \"rn_hr1\", \"hm\", \"si\", \"ta_chi\",\"heat_demand\"]\n",
    "\n",
    "\n",
    "def create_time_series_features(df, target_cols=['ta', 'ws'], freq_hours=23):  # 24â†’23, hmâ†’ws\n",
    "    \"\"\"ì‹œê³„ì—´ ë¶„í•´ë¥¼ í†µí•œ íŠ¹ì„± ìƒì„± (STL ë¶„í•´ë§Œ ì‚¬ìš©)\"\"\"\n",
    "    print(f\"ğŸ”„ STL ì‹œê³„ì—´ ë¶„í•´ íŠ¹ì„± ìƒì„± ì¤‘... (ëŒ€ìƒ: {target_cols})\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"âš ï¸ heat_demandëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì—†ìœ¼ë¯€ë¡œ ì œì™¸ë©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # ë¸Œëœì¹˜ë³„ë¡œ ì‹œê³„ì—´ ë¶„í•´ ìˆ˜í–‰\n",
    "    for branch in tqdm(sorted(df['branch_id'].unique()), desc=\"ë¸Œëœì¹˜ë³„ STL ë¶„í•´\"):\n",
    "        branch_mask = df_features['branch_id'] == branch\n",
    "        branch_data = df_features[branch_mask].copy().sort_values('tm')\n",
    "        \n",
    "        if len(branch_data) < freq_hours * 7:  # ìµœì†Œ 7ì¼ ë°ì´í„° í•„ìš”\n",
    "            print(f\"   âš ï¸ ë¸Œëœì¹˜ {branch}: ë°ì´í„° ë¶€ì¡± ({len(branch_data)}ê°œ) - ê±´ë„ˆëœ€\")\n",
    "            continue\n",
    "        \n",
    "        # ê° ëŒ€ìƒ ë³€ìˆ˜ë³„ë¡œ STL ë¶„í•´ ìˆ˜í–‰\n",
    "        for col in target_cols:\n",
    "            if col not in branch_data.columns:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "                col_data = branch_data[col].interpolate().fillna(method='bfill').fillna(method='ffill')\n",
    "                \n",
    "                # STL ë¶„í•´ (24ì‹œê°„ ì£¼ê¸°)\n",
    "                try:\n",
    "                    # ì‹œê°„ ì¸ë±ìŠ¤ ì„¤ì •\n",
    "                    ts_data = col_data.copy()\n",
    "                    ts_data.index = pd.to_datetime(branch_data['tm'])\n",
    "                    \n",
    "                    # STL ë¶„í•´\n",
    "                    stl = STL(ts_data, seasonal=freq_hours, robust=True)\n",
    "                    stl_result = stl.fit()\n",
    "                    \n",
    "                    # STL ê²°ê³¼ ì €ì¥\n",
    "                    indices = branch_data.index\n",
    "                    df_features.loc[indices, f'{col}_stl_trend'] = stl_result.trend.values\n",
    "                    df_features.loc[indices, f'{col}_stl_seasonal'] = stl_result.seasonal.values\n",
    "                    df_features.loc[indices, f'{col}_stl_resid'] = stl_result.resid.values\n",
    "                    \n",
    "                    # ì¶”ê°€ íŒŒìƒ ë³€ìˆ˜\n",
    "                    df_features.loc[indices, f'{col}_detrend'] = col_data.values - stl_result.trend.values\n",
    "                    df_features.loc[indices, f'{col}_seasonal_strength'] = np.abs(stl_result.seasonal.values)\n",
    "                    \n",
    "                    # ê³„ì ˆì„± ë³€ë™ ì§€í‘œ\n",
    "                    seasonal_std = np.std(stl_result.seasonal.values)\n",
    "                    df_features.loc[indices, f'{col}_seasonal_volatility'] = seasonal_std\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"      âš ï¸ STL ë¶„í•´ ì‹¤íŒ¨ ({col}): {str(e)[:50]}\")\n",
    "                    continue\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ ë¸Œëœì¹˜ {branch} {col} ë¶„í•´ ì‹¤íŒ¨: {str(e)[:50]}\")\n",
    "                continue\n",
    "    \n",
    "    # ìƒì„±ëœ ì‹œê³„ì—´ íŠ¹ì„± ëª©ë¡\n",
    "    time_series_features = [col for col in df_features.columns \n",
    "                           if any(pattern in col for pattern in ['_stl_', '_detrend', '_seasonal_strength', '_seasonal_volatility'])]\n",
    "    \n",
    "    print(f\"\\nâœ… STL ì‹œê³„ì—´ ë¶„í•´ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ“Š ìƒì„±ëœ ì‹œê³„ì—´ íŠ¹ì„±: {len(time_series_features)}ê°œ\")\n",
    "    print(f\"ğŸ“‹ íŠ¹ì„± ëª©ë¡: {time_series_features[:10]}{'...' if len(time_series_features) > 10 else ''}\")\n",
    "    \n",
    "    return df_features, time_series_features\n",
    "\n",
    "\n",
    "def calculate_summer_apparent_temp(ta, hm):\n",
    "    \"\"\"ì—¬ë¦„ì²  ì²´ê°ì˜¨ë„ ê³„ì‚°\"\"\"\n",
    "    try:\n",
    "        tw = ta * np.arctan(0.151977 * np.sqrt(hm + 8.313659)) \\\n",
    "             + np.arctan(ta + hm) \\\n",
    "             - np.arctan(hm - 1.676331) \\\n",
    "             + 0.00391838 * hm**1.5 * np.arctan(0.023101 * hm) \\\n",
    "             - 4.686035\n",
    "        return -0.2442 + 0.55399 * tw + 0.45535 * ta - 0.0022 * tw**2 + 0.00278 * tw * ta + 3.0\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def calculate_winter_apparent_temp(ta, ws):\n",
    "    \"\"\"ê²¨ìš¸ì²  ì²´ê°ì˜¨ë„ ê³„ì‚°\"\"\"\n",
    "    try:\n",
    "        v = ws * 3.6  # m/s â†’ km/h\n",
    "        return 13.12 + 0.6215 * ta - 11.37 * v**0.16 + 0.3965 * ta * v**0.16\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def add_apparent_temp_features(df):\n",
    "    df['month'] = df['tm'].dt.month\n",
    "    df['apparent_temp'] = df.apply(lambda row:\n",
    "        calculate_summer_apparent_temp(row['ta'], row['hm']) if 5 <= row['month'] <= 9\n",
    "        else calculate_winter_apparent_temp(row['ta'], row['ws']),\n",
    "        axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def branchwise_svr_impute(df, col, time_col='tm'):\n",
    "    df = df.copy()\n",
    "    # ì‹œê°„ ì»¬ëŸ¼ì„ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜ (timestamp, ì´ˆ ë‹¨ìœ„)\n",
    "    df['_time_num'] = pd.to_datetime(df[time_col]).astype(np.int64) // 10**9\n",
    "    # branchë³„ë¡œ SVR ë³´ê°„\n",
    "    def impute_group(g):\n",
    "        return svr_impute_series(g[col], g['_time_num'])\n",
    "    # apply ê²°ê³¼ë¥¼ ì›ë˜ ì¸ë±ìŠ¤ì— ë§ê²Œ í• ë‹¹\n",
    "    df[col] = df.groupby('branch_id', group_keys=False).apply(impute_group)\n",
    "    df = df.drop(columns=['_time_num'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_weather_data(df):\n",
    "    # ë‚ ì§œ ë³€í™˜\n",
    "    df['tm'] = pd.to_datetime(df['tm'], format='%Y%m%d%H')\n",
    "    # 1. si: 08~18ì‹œê°€ ì•„ë‹ ë•Œ -99ëŠ” 0ìœ¼ë¡œ\n",
    "    mask_outside_8_to_18 = (~df['tm'].dt.hour.between(8, 18)) & (df['si'] == -99)\n",
    "    df.loc[mask_outside_8_to_18, 'si'] = 0\n",
    "\n",
    "    # 2. wdì—ì„œ 9.9ëŠ” NaNìœ¼ë¡œ\n",
    "    df['wd'] = df['wd'].replace(9.9, np.nan)\n",
    "\n",
    "    # 3. -99 ì²˜ë¦¬\n",
    "    df.replace(-99, np.nan, inplace=True)\n",
    "\n",
    "\n",
    "    # SVR ë³´ê°„\n",
    "    df = df.sort_values(['branch_id', 'tm'])\n",
    "\n",
    "    numeric_cols = ['ta', 'wd', 'ws', 'rn_day', 'rn_hr1', 'hm', 'si', 'ta_chi', 'heat_demand']\n",
    "\n",
    "    for branch in df['branch_id'].unique():\n",
    "        print(f\"   ğŸ¢ ë¸Œëœì¹˜ {branch} SVR ë³´ê°„ ì¤‘...\", end=\" \")\n",
    "        \n",
    "        branch_mask = df['branch_id'] == branch\n",
    "        branch_data = df[branch_mask].copy()\n",
    "\n",
    "        # ì‹œê°„ íŠ¹ì„± ìƒì„±\n",
    "        branch_data['hour'] = branch_data['tm'].dt.hour\n",
    "        branch_data['day_of_year'] = branch_data['tm'].dt.dayofyear\n",
    "        branch_data['month'] = branch_data['tm'].dt.month\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            if col in branch_data.columns:\n",
    "                missing_mask = branch_data[col].isna()\n",
    "\n",
    "                if missing_mask.sum() > 0:\n",
    "                    train_mask = ~missing_mask\n",
    "\n",
    "                    # ì˜ˆì¸¡í•  ë°ì´í„° ì¤€ë¹„\n",
    "                    X_train = branch_data.loc[train_mask, ['hour', 'day_of_year', 'month']].values\n",
    "                    y_train = branch_data.loc[train_mask, col].values\n",
    "                    X_pred = branch_data.loc[missing_mask, ['hour', 'day_of_year', 'month']].values\n",
    "\n",
    "                    try:\n",
    "                        scaler_X = StandardScaler()\n",
    "                        scaler_y = StandardScaler()\n",
    "\n",
    "                        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "                        y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "\n",
    "                        svr = SVR(kernel='rbf', C=1.0, gamma='scale')\n",
    "                        svr.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "                        X_pred_scaled = scaler_X.transform(X_pred)\n",
    "                        y_pred_scaled = svr.predict(X_pred_scaled)\n",
    "                        y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "                        # ë³´ê°„ ê²°ê³¼ ë°˜ì˜\n",
    "                        df.loc[branch_mask & missing_mask, col] = y_pred\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\n   âš ï¸ {col} SVR ì‹¤íŒ¨ â†’ ì„ í˜• ë³´ê°„ ëŒ€ì²´\")\n",
    "                        df.loc[branch_mask, col] = df.loc[branch_mask, col].interpolate(method='linear')\n",
    "\n",
    "                # ë‚¨ì€ ê²°ì¸¡ ffill/bfillë¡œ ì œê±°\n",
    "                df.loc[branch_mask, col] = df.loc[branch_mask, col].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "        print(\"âœ…\")\n",
    "\n",
    "    print(\"ğŸ‰ SVR ë³´ê°„ ì™„ë£Œ (ì¡°ê±´ ì—†ì´ ì „ë¶€ ì‹œë„)\")\n",
    "    #ë³´ê°„í›„ ìŒìˆ˜ë‚˜ì˜¬ ê°€ëŠ¥ì„±ì¡´ì¬\n",
    "    df.loc[df['ta'] < 0, 'ta'] = 0\n",
    "    df.loc[df['ws'] < 0, 'ws'] = 0\n",
    "\n",
    "    # ğŸ“Œ íŒŒìƒ ë³€ìˆ˜ ìƒì„±\n",
    "    df['year'] = df['tm'].dt.year\n",
    "    df['month'] = df['tm'].dt.month\n",
    "    df['day'] = df['tm'].dt.day\n",
    "    df['hour'] = df['tm'].dt.hour\n",
    "    df['date'] = df['tm'].dt.date\n",
    "    df['weekday'] = df['tm'].dt.weekday\n",
    "    df['is_weekend'] = df['weekday'].isin([5,6]).astype(int)\n",
    "\n",
    "    # ğŸ‡°ğŸ‡· í•œêµ­ ê³µíœ´ì¼\n",
    "    kr_holidays = holidays.KR()\n",
    "    df['is_holiday'] = df['tm'].dt.date.apply(lambda x: int(x in kr_holidays))\n",
    "\n",
    "    # ğŸ•’ ì‹œê°„ ì§€ì—°\n",
    "    for lag in [1, 2, 3]:\n",
    "        df[f'ta_lag_{lag}'] = df.groupby('branch_id')['ta'].shift(lag)\n",
    "        df[f'ta_lag_{lag}'] = df.groupby('branch_id')[f'ta_lag_{lag}'].transform(\n",
    "        lambda x: x.fillna(method='bfill'))\n",
    "    # ğŸ”¥ HDD / CDD\n",
    "    df['HDD18'] = np.maximum(0, 18 - df['ta'])\n",
    "    #df['CDD18'] = np.maximum(0, df['ta'] - 18)\n",
    "    df['HDD20'] = np.maximum(0, 20 - df['ta'])\n",
    "    #df['CDD20'] = np.maximum(0, df['ta'] - 20)\n",
    "\n",
    "    df['ws_diff_6h'] = df.groupby('branch_id')['ws'].transform(lambda x: x.diff(6).bfill())\n",
    "    df['ws_diff_12h'] = df.groupby('branch_id')['ws'].transform(lambda x: x.diff(12).bfill())\n",
    "    df['ws_diff_24h'] = df.groupby('branch_id')['ws'].transform(lambda x: x.diff(24).bfill())\n",
    "\n",
    "\n",
    "\n",
    "    #ì§ì ‘ë§Œë“  ì²´ê°ì˜¨ë„\n",
    "    df = add_apparent_temp_features(df)\n",
    "\n",
    "\n",
    "    # ì§€ì ë³„ ì˜¨ë„ í¸ì°¨\n",
    "    branch_mean = df.groupby('branch_id')['ta'].transform('mean')\n",
    "    df['branch_temp_abs_deviation'] = np.abs(df['ta'] - branch_mean)\n",
    "\n",
    "\n",
    "\n",
    "    # ì´ë™ í‰ê·  (3ì‹œê°„ ë‹¨ìœ„ ìµœëŒ€ 24ì‹œê°„ = 8ê°œ)\n",
    "    for n in [3, 6, 9, 12, 15, 18, 21, 24]:\n",
    "        df[f'ta_3h_avg_{n}'] = df.groupby('branch_id')['ta'].transform(lambda x: x.rolling(n, min_periods=1).mean())\n",
    "\n",
    "    # ë¶ˆì¾Œì§€ìˆ˜\n",
    "    df['DCI'] = 0.81 * df['ta'] + 0.01 * df['hm'] * (0.99 * df['ta'] - 14.3) + 46.3\n",
    "\n",
    "    # í’ì† ëƒ‰ì§€ìˆ˜ (wchi)\n",
    "    ws_kmh = df['ws'] * 3.6  # m/s -> km/h ë³€í™˜\n",
    "    df['wchi'] = 13.12 + 0.6215 * df['ta'] - 11.37 * ws_kmh**0.16 + 0.3965 * df['ta'] * ws_kmh**0.16\n",
    "\n",
    "     # í’ì† ê³ ë ¤ ì²´ê°ì˜¨ë„ (wind chill)\n",
    "    df['wind_chill'] = 13.12 + 0.6215 * df['ta'] - 11.37 * df['ws']**0.16 + 0.3965 * df['ta'] * df['ws']**0.16\n",
    "\n",
    "    # ì‹¤íš¨ì˜¨ë„\n",
    "    df['e'] = (df['hm'] / 100) * 6.105 * np.exp((17.27 * df['ta']) / (237.7 + df['ta']))\n",
    "    df['atemphi'] = df['ta'] + 0.33 * df['e'] - 0.70 * df['ws'] - 4.00\n",
    "\n",
    "    # ì£¼ê¸°ì„± ì¸ì½”ë”©\n",
    "    df['dayofyear'] = df['tm'].dt.dayofyear\n",
    "    df['dayofmonth'] = df['tm'].dt.day\n",
    "    df['weekofyear'] = df['tm'].dt.isocalendar().week.astype(int)\n",
    "\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['dayofyear_sin'] = np.sin(2 * np.pi * df['dayofyear'] / 365)\n",
    "    df['dayofyear_cos'] = np.cos(2 * np.pi * df['dayofyear'] / 365)\n",
    "    df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "    # í•˜ë£¨ 5êµ¬ê°„\n",
    "    def time_slot(h): return int(h // 5)\n",
    "    df['hour_slot_5'] = df['hour'].apply(time_slot)\n",
    "\n",
    "\n",
    "    def compute_fft_feature(series, n=10):\n",
    "        fft_vals = np.abs(fft(series.fillna(0)))\n",
    "        s = pd.Series(fft_vals[:n], index=pd.Index([f'fft_{i}' for i in range(n)], name='fft_idx'))\n",
    "        return s\n",
    "\n",
    "    fft_cols = ['ta', 'hm', 'ws', 'ta_chi', 'apparent_temp']\n",
    "    fft_features = []\n",
    "    branch_ids = df['branch_id'].unique()\n",
    "    fft_feature_dict = {bid: {} for bid in branch_ids}\n",
    "    for col in fft_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        for branch_id in branch_ids:\n",
    "            arr = df.loc[df['branch_id'] == branch_id, col].fillna(0).values\n",
    "            fft_vals = np.abs(fft(arr))[:10]\n",
    "            for i, val in enumerate(fft_vals):\n",
    "                fft_feature_dict[branch_id][f'Nph_{col}_{i}'] = val\n",
    "                \n",
    "    # DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "    fft_features_df = pd.DataFrame.from_dict(fft_feature_dict, orient='index')\n",
    "    # ì›ë³¸ dfì™€ merge\n",
    "    df = df.merge(fft_features_df, left_on='branch_id', right_index=True, how='left')\n",
    "\n",
    "    # ê¸°ì˜¨ ì°¨ë¶„\n",
    "    df['ta_diff_6h'] = df.groupby('branch_id')['ta'].transform(lambda x: x.diff(6).bfill())\n",
    "    df['ta_diff_12h'] = df.groupby('branch_id')['ta'].transform(lambda x: x.diff(12).bfill())\n",
    "    df['ta_diff_24h'] = df.groupby('branch_id')['ta'].transform(lambda x: x.diff(24).bfill())\n",
    "\n",
    "\n",
    "    # ì¼êµì°¨\n",
    "    df['day_ta_max'] = df.groupby(['branch_id', df['tm'].dt.date])['ta'].transform('max')\n",
    "    df['day_ta_min'] = df.groupby(['branch_id', df['tm'].dt.date])['ta'].transform('min')\n",
    "    df['daily_range'] = df['day_ta_max'] - df['day_ta_min']\n",
    "\n",
    "    # ì¼êµì°¨ ë³€í™”ëŸ‰\n",
    "    df['daily_range_shift'] = df.groupby('branch_id')['daily_range'].shift(1).bfill()\n",
    "    df['daily_range_shift_ta'] = df['daily_range_shift']*df['ta']\n",
    "\n",
    "    #wsë³€í™”ëŸ‰\n",
    "    df['day_ws_max'] = df.groupby(['branch_id', df['tm'].dt.date])['ws'].transform('max')\n",
    "    df['day_ws_min'] = df.groupby(['branch_id', df['tm'].dt.date])['ws'].transform('min')\n",
    "    df['daily_range_ws'] = df['day_ws_max'] - df['day_ws_min']\n",
    "\n",
    "    # ì¼êµì°¨ ë³€í™”ëŸ‰\n",
    "    df['daily_range_shift_ws'] = df.groupby('branch_id')['daily_range_ws'].shift(1).bfill()\n",
    "\n",
    "    # í”¼í¬íƒ€ì„1\n",
    "    df['peak_time1'] = 0\n",
    "    df.loc[(df['hour'] >= 0) & (df['hour'] <= 6), 'peak_time1'] = 1\n",
    "    df.loc[(df['hour'] > 6) & (df['hour'] <= 11), 'peak_time1'] = 2\n",
    "    df.loc[(df['hour'] > 11) & (df['hour'] <= 18), 'peak_time1'] = 3\n",
    "    df.loc[(df['hour'] > 18) & (df['hour'] <= 23), 'peak_time1'] = 4\n",
    "\n",
    "    # í”¼í¬íƒ€ì„2\n",
    "    df['peak_time2'] = 0\n",
    "    df.loc[(df['hour'] >= 2) & (df['hour'] <= 10), 'peak_time2'] = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # heating season\n",
    "    df['heating_season'] = df['month'].isin([10,11,12,1, 2, 3,4]).astype(int)\n",
    "\n",
    "    # ì˜¨ë„ ë²”ì£¼í™”\n",
    "    df['temp_category20'] = pd.cut(df['ta'], bins=[-np.inf, 20, np.inf], labels=['low', 'high'])\n",
    "    df['temp_category18'] = pd.cut(df['ta'], bins=[-np.inf, 18, np.inf], labels=['low', 'high'])\n",
    "    df['temp_category16'] = pd.cut(df['ta'], bins=[-np.inf, 16, np.inf], labels=['low', 'high'])\n",
    "\n",
    "    # ì˜¤ì „/ì˜¤í›„\n",
    "    df['afternoon'] = (df['hour'] >= 12).astype(int)\n",
    "\n",
    "    # ê³„ì ˆ\n",
    "    def get_season(month):\n",
    "        return {\n",
    "            12: 'winter', 1: 'winter', 2: 'winter',\n",
    "            3: 'spring', 4: 'spring', 5: 'spring',\n",
    "            6: 'summer', 7: 'summer', 8: 'summer',\n",
    "            9: 'fall', 10: 'fall', 11: 'fall'\n",
    "        }.get(month, 'unknown')\n",
    "    df['season'] = df['month'].apply(get_season)\n",
    "\n",
    "    # í•œíŒŒ ì£¼ì˜ë³´/ê²½ë³´\n",
    "    df['cold_watch'] = (df['ta'] <= -12).astype(int)  # ì£¼ì˜ë³´\n",
    "    df['cold_warning'] = (df['ta'] <= -15).astype(int)  # ê²½ë³´\n",
    "    # ì‹œê³„ì—´ ë¶„í•´ íŠ¹ì„± ìƒì„± (heat_demand ì œì™¸)\n",
    "    print(\"ğŸš€ STL ì‹œê³„ì—´ ë¶„í•´ ì‹œì‘...\")\n",
    "    df, ts_features = create_time_series_features(df, target_cols=['ta', 'ws'])\n",
    "\n",
    "\n",
    "    # ë³€í™˜ ëŒ€ìƒ ë³€ìˆ˜\n",
    "    col = 'ta'\n",
    "    '''\n",
    "    df['ta_boxcox'] = np.nan\n",
    "    df['ta_boxcox_lambda'] = np.nan\n",
    "    df['ta_boxcox_shift'] = np.nan  # shift ê°’ë„ ì €ì¥\n",
    "    for branch, group in df.groupby('branch_id'):\n",
    "        col = 'ta'\n",
    "        min_val = group[col].min()\n",
    "        if min_val <= 0:\n",
    "            shift = abs(min_val) + 1e-4\n",
    "        else:\n",
    "            shift = 0\n",
    "        shifted = group[col] + shift\n",
    "        shifted = shifted.dropna()\n",
    "        if shifted.nunique() > 1 and len(shifted) >= 2:\n",
    "            transformed, fitted_lambda = boxcox(shifted)\n",
    "            df.loc[shifted.index, 'ta_boxcox'] = transformed\n",
    "            df.loc[shifted.index, 'ta_boxcox_lambda'] = fitted_lambda\n",
    "            df.loc[shifted.index, 'ta_boxcox_shift'] = shift\n",
    "        else:\n",
    "            df.loc[group.index, 'ta_boxcox'] = np.nan\n",
    "            df.loc[group.index, 'ta_boxcox_lambda'] = np.nan\n",
    "            df.loc[group.index, 'ta_boxcox_shift'] = shift\n",
    "\n",
    "\n",
    "    '''\n",
    "    df = df.drop(columns=['date'])\n",
    "\n",
    "\n",
    "\n",
    "    return df\n",
    "#ìƒí˜¸ì‘ìš© ì²˜ë¦¬ëª»í•¨\n",
    "#êµ°ì§‘í™”ëœ ì „ì²˜ë¦¬ ëª»í•¨\n",
    "\n",
    "\n",
    "#ì •ê·œí™” ì¼ë‹¨ min max +ì›í•«ì¸ì½”ë”©\n",
    "def scale_encode(df):\n",
    "    cat_cols = [\n",
    "        'peak_time1', 'peak_time2', 'heating_season','hour_slot_5',\n",
    "        'temp_category16', 'temp_category18', 'temp_category20','afternoon', 'season','month','day','hour']\n",
    "\n",
    "    # ë²”ì£¼í˜• ë³€ìˆ˜ categoryí™”\n",
    "    for col in cat_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    # ì›-í•« ì¸ì½”ë”©\n",
    "    df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "\n",
    "    # ì—°ì†í˜• ë³€ìˆ˜ë§Œ ì¶”ì¶œ (íƒ€ê²Ÿ, ë‚ ì§œ ë“± ì œì™¸)\n",
    "    exclude_cols = ['heat_demand','peak_time1', 'peak_time2', 'heating_season','hour_slot_5',\n",
    "        'temp_category16', 'temp_category18', 'temp_category20','afternoon', 'season','month','day','hour']\n",
    "    num_cols = [col for col in df.columns\n",
    "                if (df[col].dtype in [np.float64, np.int64]) and (col not in exclude_cols)]\n",
    "\n",
    "    # MinMaxScaler ì ìš©\n",
    "    scaler = MinMaxScaler()\n",
    "    df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "df_train = preprocess_weather_data(df_train)\n",
    "df_test = preprocess_weather_data(df_test)\n",
    "\n",
    "\n",
    "df_train = scale_encode(df_train)\n",
    "df_test = scale_encode(df_test)\n",
    "\n",
    "df_train.to_csv('df_train_prescale.csv', index=True)\n",
    "df_test.to_csv('df_test_prescale.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('df_train_prescale.csv')\n",
    "df_test = pd.read_csv('df_test_prescale.csv')\n",
    "df_train = df_train.sort_values(['branch_id', 'tm'])\n",
    "df_test = df_test.sort_values(['branch_id', 'tm'])\n",
    "\n",
    "\n",
    "\n",
    "df_train = df_train.drop(columns=['year'])\n",
    "df_train = df_train.drop(columns=['Unnamed: 0'])\n",
    "df_test = df_test.drop(columns=['year'])\n",
    "df_test = df_test.drop(columns=['Unnamed: 0'])\n",
    "df_train = df_train.set_index('tm')\n",
    "df_test = df_test.set_index('tm')\n",
    "df_train = df_train.sort_index()\n",
    "df_test = df_test.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_3fold_pipeline_with_residual(df_train, df_test, target_col='heat_demand'):\n",
    "\n",
    "    features = [col for col in df_train.columns if col != target_col]\n",
    "    X = df_train[features]\n",
    "    y = df_train[target_col]\n",
    "\n",
    "    n = len(df_train)\n",
    "    fold_size = n // 3\n",
    "\n",
    "    val_rmses = []\n",
    "    test_preds = []\n",
    "\n",
    "    print(f\"ì „ì²´ ë°ì´í„° ê¸¸ì´: {n}, Fold í¬ê¸°: {fold_size}\\n\")\n",
    "\n",
    "    for fold in range(2):  # Fold 0, 1 ìˆ˜í–‰ (3ë²ˆì§¸ëŠ” í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¶„ë¦¬)\n",
    "        train_end = fold_size * (fold + 1)\n",
    "        val_end = fold_size * (fold + 2)\n",
    "\n",
    "        X_train = X.iloc[:train_end]\n",
    "        y_train = y.iloc[:train_end]\n",
    "        X_val = X.iloc[train_end:val_end]\n",
    "        y_val = y.iloc[train_end:val_end]\n",
    "\n",
    "        print(f\"===== Fold {fold+1} =====\")\n",
    "        print(f\"Train size: {len(X_train)}, Validation size: {len(X_val)}\")\n",
    "\n",
    "        # 1) LightGBM ë² ì´ì§€ì•ˆ ìµœì í™” í•¨ìˆ˜\n",
    "        def lgb_objective(trial):\n",
    "            params = {\n",
    "                'objective': 'regression',\n",
    "                'metric': 'rmse',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 10, 300),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 200),\n",
    "                'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "                'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "                'n_estimators': 1000,\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            model = lgb.LGBMRegressor(**params)\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "            )\n",
    "            val_pred = model.predict(X_val)\n",
    "            return np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "\n",
    "        lgb_study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n",
    "        lgb_study.optimize(lgb_objective, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "        best_lgb_params = lgb_study.best_params\n",
    "        best_lgb_params.update({\n",
    "            'objective': 'huber',\n",
    "            'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'n_estimators': 1000,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        })\n",
    "\n",
    "        # 2) LightGBM ìµœì  ëª¨ë¸ í•™ìŠµ\n",
    "        lgb_model = lgb.LGBMRegressor(**best_lgb_params)\n",
    "        lgb_model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    "        )\n",
    "\n",
    "        val_pred_lgb = lgb_model.predict(X_val)\n",
    "        val_rmse_lgb = np.sqrt(mean_squared_error(y_val, val_pred_lgb))\n",
    "        print(f\"LightGBM Fold {fold+1} Validation RMSE: {val_rmse_lgb:.4f}\")\n",
    "\n",
    "        # 3) ì”ì°¨ ê³„ì‚°\n",
    "        residual_train = y_train - lgb_model.predict(X_train)\n",
    "        residual_val = y_val - val_pred_lgb\n",
    "\n",
    "        # 4) XGBoost ë² ì´ì§€ì•ˆ ìµœì í™” í•¨ìˆ˜ (foldë³„ë¡œ ë™ì¼í•˜ê²Œ ë¶„í• )\n",
    "        def xgb_objective(trial):\n",
    "            params = {\n",
    "                'objective': 'reg:pseudohubererror',\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 2, 10),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "            model.fit(\n",
    "                X_train, residual_train,\n",
    "                eval_set=[(X_val, residual_val)],\n",
    "                verbose=0\n",
    "            )\n",
    "            val_pred = model.predict(X_val)\n",
    "            return np.sqrt(mean_squared_error(residual_val, val_pred))\n",
    "\n",
    "        xgb_study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n",
    "        xgb_study.optimize(xgb_objective, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "        best_xgb_params = xgb_study.best_params\n",
    "        best_xgb_params.update({\n",
    "            'objective': 'reg:pseudohubererror',\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        })\n",
    "\n",
    "        # 5) XGBoost ìµœì  ëª¨ë¸ í•™ìŠµ (ì”ì°¨ ì˜ˆì¸¡ìš©, foldë³„ë¡œ)\n",
    "        xgb_model = xgb.XGBRegressor(**best_xgb_params)\n",
    "        xgb_model.fit(\n",
    "            X_train, residual_train,\n",
    "            eval_set=[(X_val, residual_val)],\n",
    "            verbose=100\n",
    "        )\n",
    "\n",
    "        val_pred_residual = xgb_model.predict(X_val)\n",
    "        val_pred_final = val_pred_lgb + val_pred_residual\n",
    "        val_rmse_final = np.sqrt(mean_squared_error(y_val, val_pred_final))\n",
    "        print(f\"Residual Stacking Fold {fold+1} Validation RMSE: {val_rmse_final:.4f}\")\n",
    "\n",
    "        val_rmses.append(val_rmse_final)\n",
    "\n",
    "        # foldë³„ë¡œ test ì˜ˆì¸¡ ì €ì¥\n",
    "        test_pred_lgb = lgb_model.predict(df_test[features])\n",
    "        test_pred_residual = xgb_model.predict(df_test[features])\n",
    "        test_pred_fold = test_pred_lgb + test_pred_residual\n",
    "        test_preds.append(test_pred_fold)\n",
    "\n",
    "        print(\"-----------------------------\")\n",
    "\n",
    "    avg_val_rmse = np.mean(val_rmses)\n",
    "    avg_test_pred = np.mean(test_preds, axis=0)\n",
    "\n",
    "    print(f\"\\nìµœì¢… í‰ê·  Validation RMSE: {avg_val_rmse:.4f}\")\n",
    "\n",
    "    df_test[target_col] = avg_test_pred\n",
    "\n",
    "    return {\n",
    "        'val_rmse': avg_val_rmse,\n",
    "        'val_rmses': val_rmses,\n",
    "        'test_pred': avg_test_pred,\n",
    "        'test_index': df_test.index\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_rmse_results = {}\n",
    "test_pred_dict = {}\n",
    "\n",
    "branch_ids = df_train['branch_id'].unique()\n",
    "\n",
    "for branch in branch_ids:\n",
    "    train_branch = df_train[df_train['branch_id'] == branch].copy()\n",
    "    test_branch = df_test[df_test['branch_id'] == branch].copy()\n",
    "    \n",
    "    # branch_idëŠ” ëª¨ë¸ì— ë¶ˆí•„ìš”í•˜ë©´ ì œê±°\n",
    "    train_branch = train_branch.drop(columns=['branch_id'])\n",
    "    test_branch = test_branch.drop(columns=['branch_id'])\n",
    "    \n",
    "    target_col = 'heat_demand'\n",
    "    \n",
    "    results = run_3fold_pipeline_with_residual(train_branch, test_branch, target_col)\n",
    "    \n",
    "    branch_rmse_results[branch] = {\n",
    "        'val_rmse': results['val_rmse']\n",
    "    }\n",
    "    \n",
    "    test_pred_dict[branch] = pd.DataFrame({\n",
    "        'branch_ID': branch,\n",
    "        'TM': results['test_index'],\n",
    "        'heat_demand': results['test_pred']\n",
    "    }).set_index('TM')\n",
    "\n",
    "\n",
    "# test_pred_dictì˜ ëª¨ë“  branch ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ë³‘í•©\n",
    "merged_df = pd.concat(test_pred_dict.values()).reset_index()\n",
    "\n",
    "\n",
    "# CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "merged_df.to_csv('250464_test.csv', index=False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('250464_test.csv')\n",
    "df1['TM'] = pd.to_datetime(df1['TM'])\n",
    "df1 = df1.sort_values(by=['branch_ID', 'TM']).reset_index(drop=True)\n",
    "df2 = pd.read_csv('test_heat.csv')\n",
    "df2['heat_demand'] = df1['heat_demand'].round(1)\n",
    "df2.to_csv('250464.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#huberì§€í‘œë‘ êµì°¨ê²€ì¦ ìˆœì°¨ë¡œí•˜ëŠ”ì‹ìœ¼ë¡œ 3foldë¡œí•´ì„œ ì—°ë„ë³„ë¡œ ì§œë¦„ ê¹Œì§€ ê³ ë ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
